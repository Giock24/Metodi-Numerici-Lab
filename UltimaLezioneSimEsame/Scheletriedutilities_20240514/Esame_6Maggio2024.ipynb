{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e97ec2-857d-44c0-8b7f-c9b2c436362e",
   "metadata": {},
   "source": [
    "## Esame di Metodi Numerici 6 Maggio 2024 \n",
    "\n",
    "## Esercizo 1\n",
    "- Si consideri il sistema lineare Ax=b, con A matrice e b termine noto memorizzati nel file ``'test_14_09_2023.mat'``.  Risolvere il sistema confrontando almeno due tra i metodi  visti  per utilizzare per risolvere il sistema lineare con tale matrice dei coefficienti. Confrontare i risultati dei vari metodi, e giustificare i loro comportamento utilizzando i risultati teorici visti a lezione.\n",
    "- \n",
    "Per la lettura dei dati procedere nel seguente modo:\n",
    "\n",
    "``from scipy.io import loadmat``\n",
    "\n",
    "``import numpy as np``\n",
    "\n",
    "``dati = loadmat('test_06_05_2024.mat')``\n",
    "\n",
    "``A=dati[\"A\"] ``\n",
    "\n",
    "``A=A.astype(float)``\n",
    "\n",
    "`` b=dati[\"b\"] ``\n",
    "\n",
    "`` b=b.astype(float)``\n",
    "\n",
    "\n",
    "                                       [10 punti]\n",
    "                                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d3c69ed-78c7-41b0-add7-029b50293ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n",
      "1.2449999999999999 %\n",
      "True\n",
      "True\n",
      "True\n",
      "iterazioni gradiente  107\n",
      "iterazioni gradiente coniugato  36\n",
      "velocità del gradiente  0.8796121538543219\n",
      "velocità del coniugato  0.5960677546812206\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "dati = loadmat('test_06_05_2024.mat')\n",
    "A=dati[\"A\"] \n",
    "A=A.astype(float)\n",
    "b=dati[\"b\"]\n",
    "b=b.astype(float)\n",
    "\n",
    "print(A.shape)\n",
    "print((np.count_nonzero(A)/(A.shape[0]*A.shape[1]))*100,\"%\")\n",
    "elem = np.sum(np.array(A[0,1:A.shape[0]]))\n",
    "print(elem < A[0][0]) # ha diagonale strettamente dominante\n",
    "\n",
    "# controllo simmetrica\n",
    "print(A.all() == A.T.all())\n",
    "\n",
    "# controllo se è definita positiva\n",
    "autovalori = np.linalg.eigvals(A)\n",
    "print(autovalori.all() > 0)\n",
    "\n",
    "def steepestdescent(A,b,x0,itmax,tol):\n",
    " \n",
    "    n,m=A.shape\n",
    "    if n!=m:\n",
    "        print(\"Matrice non quadrata\")\n",
    "        return [],[]\n",
    "    \n",
    "    \n",
    "   # inizializzare le variabili necessarie\n",
    "    x = x0\n",
    "\n",
    "     \n",
    "    r = A@x-b\n",
    "    p = -r\n",
    "    it = 0\n",
    "    nb=np.linalg.norm(b)\n",
    "    errore=np.linalg.norm(r)/nb\n",
    "    vec_sol=[]\n",
    "    vec_sol.append(x)\n",
    "    vet_r=[]\n",
    "    vet_r.append(errore)\n",
    "     \n",
    "    # utilizzare il metodo del gradiente per trovare la soluzione\n",
    "    while it < itmax and errore > toll:\n",
    "        it=it+1\n",
    "        Ap= A@p\n",
    "        alpha = (r.T@r)/((A@r).T@r)\n",
    "                \n",
    "        x = x + alpha*p   \n",
    "        vec_sol.append(x)\n",
    "        r=r+alpha*Ap\n",
    "        errore=np.linalg.norm(r)/nb\n",
    "        vet_r.append(errore)\n",
    "        p = -r\n",
    "        \n",
    "     \n",
    "    return x,vet_r,vec_sol,it\n",
    "\n",
    "def conjugate_gradient(A,b,x0,itmax,tol):\n",
    "    n,m=A.shape\n",
    "    if n!=m:\n",
    "        print(\"Matrice non quadrata\")\n",
    "        return [],[]\n",
    "    \n",
    "    \n",
    "   # inizializzare le variabili necessarie\n",
    "    x = x0\n",
    "    \n",
    "    r = A@x-b\n",
    "    p = -r\n",
    "    it = 0\n",
    "    nb=np.linalg.norm(b)\n",
    "    errore=np.linalg.norm(r)/nb\n",
    "    vec_sol=[]\n",
    "    vec_sol.append(x0)\n",
    "    vet_r=[]\n",
    "    vet_r.append(errore)\n",
    "    # utilizzare il metodo del gradiente coniugato per calcolare la soluzione\n",
    "    while errore >= tol and it< itmax:\n",
    "        it=it+1\n",
    "        Ap=A@p\n",
    "        alpha = (r.T@r)/((Ap).T@p)\n",
    "        x = x + alpha*p\n",
    "        vec_sol.append(x)\n",
    "        rtr_old=r.T@r\n",
    "        r=r+alpha*Ap\n",
    "        gamma= (r.T@r)/rtr_old\n",
    "        errore=np.linalg.norm(r)/nb\n",
    "        vet_r.append(errore)\n",
    "        p =  -r + gamma*p\n",
    "   \n",
    "    \n",
    "    return x,vet_r,vec_sol,it\n",
    "\n",
    "toll=1e-10\n",
    "maxit=4000\n",
    "x0 = np.zeros_like(b)\n",
    "\n",
    "xC,vet_rC,vec_solC,itC = steepestdescent(A,b,x0,maxit,toll)\n",
    "print(\"iterazioni gradiente \", itC)\n",
    "xG,vet_rG,vec_solG,itG = conjugate_gradient(A,b,x0,maxit,toll)\n",
    "print(\"iterazioni gradiente coniugato \", itG)\n",
    "\n",
    "# calcolo la velocità di convergenza\n",
    "\n",
    "vel_grad = ((np.linalg.cond(A)-1)/(np.linalg.cond(A)+1))*vet_rC[0]\n",
    "print(\"velocità del gradiente \", vel_grad)\n",
    "vel_coniugato = ((np.sqrt(np.linalg.cond(A))-1)/(np.sqrt(np.linalg.cond(A))+1))*vet_rG[0]\n",
    "print(\"velocità del coniugato \", vel_coniugato)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0d44a-0d01-44c1-b47f-9a155608f40b",
   "metadata": {},
   "source": [
    "- Data la matrice \n",
    "$$A=\\left[\n",
    "\\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4\\\\\n",
    "2 & -4 & 6 & 8\\\\\n",
    "-1 & -2 & -3 & -1\\\\\n",
    "5 & 7 & 0 & 1\n",
    "\\end{array}\n",
    "\\right ],$$\n",
    "Richiamare le ipotesi sotto cui esiste la fattorizzazione di Gauss senza pivoting e scrivere un codice per  verificarle.\n",
    "\n",
    "                                                [2 punti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09c38d95-2dc1-4196-a4eb-0a3b38b532b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "93.75 %\n",
      "8.921283937729623\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lu\n",
    "import SolveTriangular\n",
    "\n",
    "# non serviva\n",
    "#def gauss(A, b):\n",
    "#    PT, L, U = lu(A)\n",
    "#    P = P.T\n",
    "#    \n",
    "#    y, flag = SolveTriangular.Lsolve(L, P@b)\n",
    "#    x, flag = SolveTriangular.Usolve(U, y)\n",
    "#    \n",
    "#    return x\n",
    "\n",
    "A2 = np.array([ [1,2,3,4], [2,-4,6,8], [-1,-2,-3,-1], [5,7,0,1] ])\n",
    "\n",
    "print(A2.shape) # è quadrata ed è di piccole dimensioni\n",
    "print(np.count_nonzero(A2)/(A2.shape[0]*A2.shape[1])*100,\"%\") # è densa\n",
    "print(np.linalg.cond(A2)) # è ben condizionata < 10^3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbda924-2d3b-4357-bf34-cb936d3a4a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Esercizio 2\n",
    "Scrivere uno script che calcoli il polinomio interpolante un insieme di punti $P_i =(x_i, y_i)$ $i = 0, ..., n $ nella forma di Lagrange, $n=5,10,15,18$\n",
    "\n",
    "- nodi $x_i$, punti equidistanti in un intervallo $[a, b]$,\n",
    "- nodi $x_i$, zeri dei polinomi di Chebyshev nell'intervallo $[a, b]$, ossia\n",
    "$$\n",
    "x_i = \\frac{(a + b)}{2}+\\frac{(b-a)}{2} \\, \\cos \\left(\n",
    "\\frac{(2i+1)\\pi}{2(n + 1)}\n",
    "\\right), \\quad  i =0, ..., n \n",
    "$$\n",
    " \n",
    "  e $y_i = f(x_i)$ ottenuti dalla valutazione nei punti $x_i$ della funzione test   $f: \\ [a, b] \\rightarrow {\\mathbb R}$. \n",
    "  - $f(x) = 1/(1+25*x^2)$,  $ \\quad x \\in [-1, 1]$ (funzione di Runge).\n",
    "  \n",
    "                                          [6] punti\n",
    "\n",
    "- Calcolare l'errore di interpolazione $r(x) =  f(x)-pe(x) $,\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione $pe(x)$ calcolato a partire da nodi equdisitanti.\n",
    "                                        [1] punto\n",
    "                                        \n",
    "Visualizzare il grafico di $f(x)$ e $pe(x)$, ed il grafico di $|r(x)|$ per ogni valore $n=5,10,15,18$ \n",
    "\n",
    "                                        [1] punto\n",
    "                                        \n",
    "Calcolare l'errore di interpolazione $r(x) =  f(x)-pc(x) $,\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione $p(x)$ calcolato a partire da nodi di Chebichev.\n",
    "\n",
    "                                      [1] punto\n",
    "                                            \n",
    "Visualizzare il grafico di $f(x)$ e $pc(x)$, ed il grafico di $|r(x)|$. \n",
    "\n",
    "                                       [1] punto\n",
    "\n",
    "Cosa si osserva? Cosa accade all'aumentare del grado $n$ di $p(x)$? Scrivere la formula dell'errore che si compie quando al posto della funzione che ha generato i dati si considera il polinomio interpolatore di grado n e commentarla.\n",
    "                                         \n",
    "                                         [3 punti]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4239d5c-a24b-432f-9903-e5d5e67f0669",
   "metadata": {},
   "source": [
    "**Domanda AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721a2f9-b321-4904-a015-98e1ff91f89c",
   "metadata": {},
   "source": [
    "- Descrivere gli elementi caratterizzanti di un MultiLayer Perceptron (MLP).( Com'è fatto un neurone artificiale, a caso servono le funzioni di attivazione, come sono organizzati i neuroni. Varie tipologie di reti MLP)  ed accennare in cosa consiste la fase di forward propagation e la fase di backward propagation. **Punti: 1**\n",
    "\n",
    "- Ottimizzazione della loss function per il training di una rete neurale per il task di regressione: Metodo di discesa del gradiente, metodo stocastico del gradiente, metodo del gradiente minibatch.  **Punti 1**  \n",
    " - Non convessità della loss-function - come non rimanere bloccati in un monimo locale? Metodo del gradiente con momentum. **Punti 2**\n",
    "- Learning rate scheduling: step decay, decadimento esponenziale, decadimento dipendente dal tempo. **Punti 1**\n",
    " - Learning rate adattivo: Adagrad, RMSProp, Adadelta, Adam. **Punti 2**\n",
    " \n",
    " **Totale:  7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95c228-c321-4608-9f05-a6364a46465c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
